{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. SVM classification with CV 2\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "* Improve setup from III.SVM_Sentencetraining\n",
    "* Do not do hyperparameter tuning?\n",
    "\n",
    "\n",
    "### Comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical features\n",
    "\n",
    "* token n-gram features: unigrams,bigrams, trigrams\n",
    "* character n-gram fatures: trigrams,fourgrams\n",
    "* lemma n-gram features: uni,bi,trigrams\n",
    "* disambiguated lemmas: Lemma + POS tag\n",
    "* numerals: yes,no\n",
    "* symbols: yes,no\n",
    "* time indicators: yes, not\n",
    "* future: add semantic knowledge from structured resources:\n",
    "    * takeover=acquire, acquisition,\n",
    "    * are word embedding sufficient to capture semantic knowledge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic features\n",
    "* PoS categories: \n",
    "    * for each binary (yes,no)\n",
    "    * 0,1,more; \n",
    "    * total number of occurances\n",
    "* named entity types: person, organization, location, product, event, \n",
    "\n",
    "\n",
    "    NE Type \tExamples\n",
    "    ORGANIZATION \tGeorgia-Pacific Corp., WHO\n",
    "    PERSON \tEddy Bonte, President Obama\n",
    "    LOCATION \tMurray River, Mount Everest\n",
    "    DATE \tJune, 2008-06-29\n",
    "    TIME \ttwo fifty a m, 1:30 p.m.\n",
    "    MONEY \t175 million Canadian Dollars, GBP 10.40\n",
    "    PERCENT \ttwenty pct, 18.75 %\n",
    "    FACILITY \tWashington Monument, Stonehenge\n",
    "    GPE \tSouth East Asia, Midlothian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classification\n",
    "* linear SVM\n",
    "* kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction on sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christian/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/christian/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk,gensim,spacy,re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " # load spacy's English language models\n",
    "en_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to download data\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('tagsets')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.data import load\n",
    "all_pos_tags = list(load('/Users/christian/nltk_data/help/tagsets/upenn_tagset.pickle').keys())\n",
    "#all_pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract_other_lexical_features(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    other lexical features such as time, special chars\n",
    "    '''\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self    \n",
    "\n",
    "    def transform(self, sentences):\n",
    "    \n",
    "        def extract_other_lexical_features(sentence):\n",
    "        \n",
    "            tokentext = nltk.word_tokenize(sentence)\n",
    "\n",
    "            ## Check if it is digit, could also use POS tag 'NUM'\n",
    "            digits = np.any([token.isdigit() for token in tokentext])\n",
    "            #digits = [any(char.isdigit() for char in token) for token in tokentext] #any char contains digit\n",
    "\n",
    "            ## contains symbols (true), other characters\n",
    "            symbols = np.any([not token.isalnum() for token in tokentext])\n",
    "\n",
    "            ## contains time indicators ('yesterday','today')\n",
    "            time_indicator_list = ['yesterday','today','tomorrow']\n",
    "            times = np.any([True if token in time_indicator_list else False for token in tokentext])\n",
    "            \n",
    "            return [digits,symbols,times] #{'digits':digits,'symbols':symbols,'times':times}\n",
    "        \n",
    "        return [extract_other_lexical_features(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any([False,False,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_types = ['ORGANIZATION','PERSON','LOCATION','DATE','TIME','MONEY','PERCENT','FACILITY','FACILITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract_syntactic_features(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    each sub-feature vector is of length all_pos_tags, fixed vector lengths!\n",
    "    '''\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self    \n",
    "\n",
    "    def transform(self, sentences):\n",
    "    \n",
    "        def extract_syntactic_features(sentence):\n",
    "            tokentext = nltk.word_tokenize(sentence)\n",
    "            tags = [token[1] for token in nltk.pos_tag(tokentext)]\n",
    "\n",
    "            # binary occurance of tags\n",
    "            tag_occurance = [apt in tags for apt in all_pos_tags]\n",
    "\n",
    "            count_dict = Counter(tags)\n",
    "\n",
    "            # number of occurances\n",
    "            tag_counts = [count_dict[apt] if apt in count_dict.keys() else 0 for apt in all_pos_tags]\n",
    "\n",
    "            # occurance, 0, 1 or more\n",
    "            tag_three_classes = [2 if tc>1 else tc for tc in tag_counts]\n",
    "\n",
    "            # named entity recognition: person, organization, location, product, event,\n",
    "            ner_found=[]\n",
    "            for chunk in nltk.ne_chunk(nltk.pos_tag(tokentext)):\n",
    "                if hasattr(chunk, 'label'):\n",
    "                    ner_found.append(chunk.label())\n",
    "            ners = [1 if ner in ner_found else 0 for ner in NER_types]\n",
    "\n",
    "            return tag_occurance+tag_three_classes+tag_three_classes #{'tag_occurance':tag_occurance,'tag_three_classes':tag_three_classes,'ners':ners}\n",
    "\n",
    "        return [extract_syntactic_features(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lemmatize(sentence):\n",
    "    \n",
    "    #tokentext = nltk.word_tokenize(sentence)\n",
    "    return [token.lemma_ for token in en_nlp(sentence)]\n",
    "\n",
    "def tokenize_lemma_pos(sentence):\n",
    "    '''\n",
    "    Combine token name and pos label\n",
    "    '''\n",
    "    tokentext = nltk.word_tokenize(sentence)\n",
    "    return [en_nlp(token[0])[0].lemma_+token[1] for token in nltk.pos_tag(tokentext)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.shape = X.shape\n",
    "        # what other output you want\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sentence = 'The New York Times posted about people running marathons'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    \n",
    "   # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            \n",
    "            # Pipeline for getting syntactic features\n",
    "            ('syntactic_features', Pipeline([\n",
    "                ('extract_syntactic_features', extract_syntactic_features())\n",
    "                #('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),    \n",
    "    \n",
    "\n",
    "            # Pipeline for getting other lexical features\n",
    "            ('other_lexical_features', Pipeline([\n",
    "                ('extract_other_lexical_features', extract_other_lexical_features())\n",
    "                #('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),               \n",
    "    \n",
    "            # word token ngrams\n",
    "            ('word_ngrams', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,3),analyzer='word')),\n",
    "                (\"debug\", Debug())\n",
    "            ])),\n",
    "    \n",
    "    \n",
    "            # character token ngrams\n",
    "            ('char_ngrams', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(3,4),analyzer='char')),\n",
    "                (\"debug\", Debug())\n",
    "            ])),     \n",
    "    \n",
    "            \n",
    "    \n",
    "            ## lemma n-gram features\n",
    "            ('lemma_ngrams', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(tokenizer=tokenize_lemmatize,ngram_range=(1,3),analyzer='word')),\n",
    "                (\"debug\", Debug())\n",
    "            ])),              \n",
    "    \n",
    "            \n",
    "            \n",
    "            ## Get lemma + POS tags\n",
    "            ('lemma_pos', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(tokenizer=tokenize_lemma_pos,analyzer='word')),\n",
    "                (\"debug\", Debug()),\n",
    "            ]))     \n",
    "            \n",
    "\n",
    "        ]\n",
    "        \n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "        #    'subject': 1.0,\n",
    "        #    'body_bow': 1.0,\n",
    "        #    'body_stats': 1.0,\n",
    "        #},\n",
    "    )),\n",
    "    # Use a SVC classifier on the combined features\n",
    "    #('svc', SVC(kernel='linear')),\n",
    "    \n",
    "    (\"debug_final\", Debug()),\n",
    "\n",
    "    ('svc',OneVsRestClassifier(SVC(kernel='linear'))),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('jacobs_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentences</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>file_id</th>\n",
       "      <th>-1</th>\n",
       "      <th>Profit</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>MergerAcquisition</th>\n",
       "      <th>SalesVolume</th>\n",
       "      <th>BuyRating</th>\n",
       "      <th>QuarterlyResults</th>\n",
       "      <th>TargetPrice</th>\n",
       "      <th>ShareRepurchase</th>\n",
       "      <th>Turnover</th>\n",
       "      <th>Debt</th>\n",
       "      <th>-1.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>It will not say what it has spent on the proje...</td>\n",
       "      <td>tesco</td>\n",
       "      <td>25-09-2013</td>\n",
       "      <td>569</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>Sir John Bond , chairman , told the bank 's an...</td>\n",
       "      <td>FT other HSBC</td>\n",
       "      <td>28-05-2005</td>\n",
       "      <td>425</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          sentences          title  \\\n",
       "0    -1  It will not say what it has spent on the proje...          tesco   \n",
       "1    -1  Sir John Bond , chairman , told the bank 's an...  FT other HSBC   \n",
       "\n",
       "  publication_date  file_id  -1  Profit  Dividend  MergerAcquisition  \\\n",
       "0       25-09-2013      569   1       0         0                  0   \n",
       "1       28-05-2005      425   1       0         0                  0   \n",
       "\n",
       "   SalesVolume  BuyRating  QuarterlyResults  TargetPrice  ShareRepurchase  \\\n",
       "0            0          0                 0            0                0   \n",
       "1            0          0                 0            0                0   \n",
       "\n",
       "   Turnover  Debt  -1.1  \n",
       "0         0     0     0  \n",
       "1         0     0     0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_labels = ['-1',\n",
    "       'Profit', 'Dividend', 'MergerAcquisition', 'SalesVolume', 'BuyRating',\n",
    "       'QuarterlyResults', 'TargetPrice', 'ShareRepurchase', 'Turnover',\n",
    "       'Debt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9937,), (9937, 11))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['sentences']\n",
    "y = df[multi_labels]\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,file_id_train,file_id_test = train_test_split(X, y, df['file_id'],test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490, 3280)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_id_test.nunique(),len(file_id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 6657)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_id_train.nunique(),len(file_id_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which documents do not appear in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46min 23s, sys: 38.2 s, total: 47min 1s\n",
      "Wall time: 27min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('syntactic_features', Pipeline(memory=None,\n",
       "     steps=[('extract_syntactic_features', extract_syntactic_features())])), ('other_lexical_features', Pipeline(memory=None,\n",
       "     steps=[('extract_other_lexical_features', extract_other_lex...ability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          n_jobs=1))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### svae model in json?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline.joblib']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, 'pipeline.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'jsonpickle.ext.numpy' has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-389d9f051962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjsonpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjsonpickle_numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mjsonpickle_numpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_handlers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfrozen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsonpickle_numpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#thawed = jsonpickle.decode(frozen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'jsonpickle.ext.numpy' has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "import jsonpickle.ext.numpy as jsonpickle_numpy\n",
    "jsonpickle_numpy.register_handlers()\n",
    "frozen = jsonpickle.encode(pipeline)\n",
    "#thawed = jsonpickle.decode(frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('union', FeatureUnion(n_jobs=1,\n",
       "         transformer_list=[('syntactic_features', Pipeline(memory=None,\n",
       "       steps=[('extract_syntactic_features', extract_syntactic_features())])), ('other_lexical_features', Pipeline(memory=None,\n",
       "       steps=[('extract_other_lexical_features', extract_other_lexical_features())])), ('word_ngrams', Pipeline(m... tokenize_lemma_pos at 0x135d26b70>,\n",
       "          use_idf=True, vocabulary=None)), ('debug', Debug())]))],\n",
       "         transformer_weights=None)),\n",
       " ('debug_final', Debug()),\n",
       " ('svc',\n",
       "  OneVsRestClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "            n_jobs=1))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()['svc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6657, 13745), (6657, 176571), (6657, 43589), (6657, 183262))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()['union'].get_params()['lemma_pos'].get_params()['debug'].shape, \\\n",
    "pipeline.get_params()['union'].get_params()['word_ngrams'].get_params()['debug'].shape,\\\n",
    "pipeline.get_params()['union'].get_params()['char_ngrams'].get_params()['debug'].shape,\\\n",
    "pipeline.get_params()['union'].get_params()['lemma_ngrams'].get_params()['debug'].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 400k features per data point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6657, 417305)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()['debug_final'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "               -1       0.92      0.95      0.94      2593\n",
      "           Profit       0.83      0.77      0.80       218\n",
      "         Dividend       0.70      0.70      0.70        50\n",
      "MergerAcquisition       0.71      0.21      0.32        81\n",
      "      SalesVolume       0.79      0.73      0.76       143\n",
      "        BuyRating       0.95      0.72      0.82        72\n",
      " QuarterlyResults       0.68      0.74      0.71        88\n",
      "      TargetPrice       0.87      0.96      0.92        28\n",
      "  ShareRepurchase       0.83      0.38      0.53        26\n",
      "         Turnover       0.86      0.64      0.73        77\n",
      "             Debt       0.55      0.30      0.39        20\n",
      "\n",
      "      avg / total       0.89      0.88      0.88      3396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred,target_names=multi_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
