{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. CNN classification 3\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "* Test performance of Spacy CNN\n",
    "\n",
    "\n",
    "### Comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction on sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk,gensim, spacy\n",
    "\n",
    "#nltk.data.path=[]\n",
    "#nltk.data.path.append(\"C:\\\\Users\\\\rittchr\\\\nltk_data\")\n",
    "#nltk.data.path.append(\"\\\\esdfiles\\INTERNAL\\SpecialProjects\\EconomicEventDetection\\Analytics\\nltk_data\")\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    \n",
    "   # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            \n",
    "            # Pipeline for getting syntactic features\n",
    "            ('syntactic_features', Pipeline([\n",
    "                ('extract_syntactic_features', extract_syntactic_features())\n",
    "                #('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),    \n",
    "    \n",
    "\n",
    "            # Pipeline for getting other lexical features\n",
    "            ('other_lexical_features', Pipeline([\n",
    "                ('extract_other_lexical_features', extract_other_lexical_features())\n",
    "                #('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),               \n",
    "    \n",
    "            # word token ngrams\n",
    "            ('word_ngrams', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,3),analyzer='word')),\n",
    "                (\"debug\", Debug())\n",
    "            ])),\n",
    "    \n",
    "    \n",
    "            # character token ngrams\n",
    "            ('char_ngrams', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(3,4),analyzer='char')),\n",
    "                (\"debug\", Debug())\n",
    "            ])),     \n",
    "    \n",
    "            \n",
    "    \n",
    "            ## lemma n-gram features, MODIFIED tokenizer=tokenize_lemmatize,\n",
    "            ('lemma_ngrams', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,3),analyzer='word')),\n",
    "                (\"debug\", Debug())\n",
    "            ])),              \n",
    "    \n",
    "            \n",
    "            \n",
    "            ## Get lemma + POS tags, MODIFIED, tokenizer=tokenize_lemma_pos,\n",
    "            ('lemma_pos', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(analyzer='word')),\n",
    "                (\"debug\", Debug()),\n",
    "            ]))     \n",
    "            \n",
    "\n",
    "        ]\n",
    "        \n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "        #    'subject': 1.0,\n",
    "        #    'body_bow': 1.0,\n",
    "        #    'body_stats': 1.0,\n",
    "        #},\n",
    "    )),\n",
    "    # Use a SVC classifier on the combined features\n",
    "    #('svc', SVC(kernel='linear')),\n",
    "    \n",
    "    (\"debug_final\", Debug()),\n",
    "\n",
    "    ('svc',OneVsRestClassifier(SVC(kernel='linear'))),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_corpus = './Data/jacobs_corpus.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(path_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>datatype</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>file_id</th>\n",
       "      <th>-1</th>\n",
       "      <th>Profit</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>MergerAcquisition</th>\n",
       "      <th>SalesVolume</th>\n",
       "      <th>BuyRating</th>\n",
       "      <th>QuarterlyResults</th>\n",
       "      <th>TargetPrice</th>\n",
       "      <th>ShareRepurchase</th>\n",
       "      <th>Turnover</th>\n",
       "      <th>Debt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It will not say what it has spent on the proje...</td>\n",
       "      <td>-1</td>\n",
       "      <td>holdin</td>\n",
       "      <td>tesco</td>\n",
       "      <td>25-09-2013</td>\n",
       "      <td>833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sir John Bond , chairman , told the bank 's an...</td>\n",
       "      <td>-1</td>\n",
       "      <td>holdin</td>\n",
       "      <td>FT other HSBC</td>\n",
       "      <td>28-05-2005</td>\n",
       "      <td>393</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence label datatype  \\\n",
       "0  It will not say what it has spent on the proje...    -1   holdin   \n",
       "1  Sir John Bond , chairman , told the bank 's an...    -1   holdin   \n",
       "\n",
       "           title publication_date  file_id  -1  Profit  Dividend  \\\n",
       "0          tesco       25-09-2013      833   1       0         0   \n",
       "1  FT other HSBC       28-05-2005      393   1       0         0   \n",
       "\n",
       "   MergerAcquisition  SalesVolume  BuyRating  QuarterlyResults  TargetPrice  \\\n",
       "0                  0            0          0                 0            0   \n",
       "1                  0            0          0                 0            0   \n",
       "\n",
       "   ShareRepurchase  Turnover  Debt  \n",
       "0                0         0     0  \n",
       "1                0         0     0  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_labels = ['-1',\n",
    "       'Profit', 'Dividend', 'MergerAcquisition', 'SalesVolume', 'BuyRating',\n",
    "       'QuarterlyResults', 'TargetPrice', 'ShareRepurchase', 'Turnover',\n",
    "       'Debt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9937,), (9937, 11))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['sentence']\n",
    "y = df[multi_labels]\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[231,233],[123],[2,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 231, 233, 123]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(x for l in a for x in l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tags:  ['ADP', 'SCONJ', 'PUNCT', 'PRON', 'SYM', 'VERB', 'AUX', 'NOUN', 'PROPN', 'X', 'ADJ', 'CCONJ', 'NUM', 'INTJ', 'SPACE', 'DET', 'PART', 'ADV']\n",
      "uniquener:  ['GPE', 'DATE', 'ORG', 'TIME', 'ORDINAL', 'LOC', 'FAC', 'PERSON', 'WORK_OF_ART', 'NORP', 'EVENT', 'MONEY', 'CARDINAL', 'LAW', 'PRODUCT', 'PERCENT', 'QUANTITY', 'LANGUAGE']\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('union',\n",
       "  FeatureUnion(n_jobs=None,\n",
       "               transformer_list=[('syntactic_features',\n",
       "                                  Pipeline(memory=None,\n",
       "                                           steps=[('extract_syntactic_features',\n",
       "                                                   extract_syntactic_features())],\n",
       "                                           verbose=False)),\n",
       "                                 ('other_lexical_features',\n",
       "                                  Pipeline(memory=None,\n",
       "                                           steps=[('extract_other_lexical_features',\n",
       "                                                   extract_other_lexical_features())],\n",
       "                                           verbose=False)),\n",
       "                                 ('word_ngrams',\n",
       "                                  Pipeline(memory=None,\n",
       "                                           steps...\n",
       "                                                                   encoding='utf-8',\n",
       "                                                                   input='content',\n",
       "                                                                   lowercase=True,\n",
       "                                                                   max_df=1.0,\n",
       "                                                                   max_features=None,\n",
       "                                                                   min_df=1,\n",
       "                                                                   ngram_range=(1,\n",
       "                                                                                1),\n",
       "                                                                   norm='l2',\n",
       "                                                                   preprocessor=None,\n",
       "                                                                   smooth_idf=True,\n",
       "                                                                   stop_words=None,\n",
       "                                                                   strip_accents=None,\n",
       "                                                                   sublinear_tf=False,\n",
       "                                                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                   tokenizer=None,\n",
       "                                                                   use_idf=True,\n",
       "                                                                   vocabulary=None)),\n",
       "                                                  ('debug', Debug())],\n",
       "                                           verbose=False))],\n",
       "               transformer_weights=None, verbose=False)),\n",
       " ('debug_final', Debug()),\n",
       " ('svc',\n",
       "  OneVsRestClassifier(estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                    class_weight=None, coef0=0.0,\n",
       "                                    decision_function_shape='ovr', degree=3,\n",
       "                                    gamma='scale', kernel='linear', max_iter=-1,\n",
       "                                    probability=False, random_state=None,\n",
       "                                    shrinking=True, tol=0.001, verbose=False),\n",
       "                      n_jobs=None))]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()['svc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6657, 10264), (6657, 176571), (6657, 43589), (6657, 176571))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()['union'].get_params()['lemma_pos'].get_params()['debug'].shape, \\\n",
    "pipeline.get_params()['union'].get_params()['word_ngrams'].get_params()['debug'].shape,\\\n",
    "pipeline.get_params()['union'].get_params()['char_ngrams'].get_params()['debug'].shape,\\\n",
    "pipeline.get_params()['union'].get_params()['lemma_ngrams'].get_params()['debug'].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 400k features per data point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6657, 407133)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()['debug_final'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "               -1       0.92      0.95      0.94      2593\n",
      "           Profit       0.81      0.76      0.78       218\n",
      "         Dividend       0.68      0.64      0.66        50\n",
      "MergerAcquisition       0.65      0.21      0.32        81\n",
      "      SalesVolume       0.80      0.73      0.76       143\n",
      "        BuyRating       0.94      0.69      0.80        72\n",
      " QuarterlyResults       0.68      0.74      0.71        88\n",
      "      TargetPrice       0.86      0.89      0.88        28\n",
      "  ShareRepurchase       0.79      0.42      0.55        26\n",
      "         Turnover       0.88      0.66      0.76        77\n",
      "             Debt       0.62      0.40      0.48        20\n",
      "\n",
      "      avg / total       0.89      0.88      0.88      3396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred,target_names=multi_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Models/TrainingJacobs/model.joblib']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline,'../Models/TrainingJacobs/model.joblib',compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('py36': conda)",
   "language": "python",
   "name": "python36764bitpy36conda5bb0a2a1b1794bc58c6e05827e429966"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
