{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. SVM classification 3\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "* Improve setup from III.SVM_Sentencetraining2\n",
    "* Adapted to work with latest version of spacy (03/20)\n",
    "\n",
    "### Comments\n",
    "\n",
    "* For branch opening the mentioning of a time could be relevant. How often is this happening?\n",
    "* We check for numerals, symbols and time indicators only on the sentence level! yes/no. Should we be preciser?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical features\n",
    "\n",
    "* token n-gram features: unigrams,bigrams, trigrams\n",
    "* character n-gram fatures: trigrams,fourgrams\n",
    "* lemma n-gram features: uni,bi,trigrams\n",
    "* disambiguated lemmas: Lemma + POS tag\n",
    "* numerals: yes,no\n",
    "* symbols: yes,no\n",
    "* time indicators: yes, not\n",
    "* future: add semantic knowledge from structured resources:\n",
    "    * takeover=acquire, acquisition,\n",
    "    * are word embedding sufficient to capture semantic knowledge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic features\n",
    "* PoS categories: \n",
    "    * for each binary (yes,no)\n",
    "    * 0,1,more; \n",
    "    * total number of occurances\n",
    "* named entity types: person, organization, location, product, event, \n",
    "\n",
    "\n",
    "    NE Type \tExamples\n",
    "    ORGANIZATION \tGeorgia-Pacific Corp., WHO\n",
    "    PERSON \tEddy Bonte, President Obama\n",
    "    LOCATION \tMurray River, Mount Everest\n",
    "    DATE \tJune, 2008-06-29\n",
    "    TIME \ttwo fifty a m, 1:30 p.m.\n",
    "    MONEY \t175 million Canadian Dollars, GBP 10.40\n",
    "    PERCENT \ttwenty pct, 18.75 %\n",
    "    FACILITY \tWashington Monument, Stonehenge\n",
    "    GPE \tSouth East Asia, Midlothian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction on sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk,gensim, spacy\n",
    "\n",
    "#nltk.data.path=[]\n",
    "#nltk.data.path.append(\"C:\\\\Users\\\\rittchr\\\\nltk_data\")\n",
    "#nltk.data.path.append(\"\\\\esdfiles\\INTERNAL\\SpecialProjects\\EconomicEventDetection\\Analytics\\nltk_data\")\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('tagsets')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.data import load\n",
    "#all_pos_tags = list(load('help/tagsets/upenn_tagset.pickle').keys())\n",
    "#all_pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract_other_lexical_features(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    other lexical features such as time, special chars\n",
    "    '''\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self    \n",
    "\n",
    "    def transform(self, sentences):\n",
    "    \n",
    "        def extract_other_lexical_features_int(sentence):\n",
    "            '''\n",
    "            Simple indicator variables if digits, symobls or times are mentioned in the sentence\n",
    "            '''\n",
    "        \n",
    "            tokentext = nltk.word_tokenize(sentence)\n",
    "\n",
    "            ## Check if it is digit, could also use POS tag 'NUM'\n",
    "            digits = np.any([token.isdigit() for token in tokentext])\n",
    "            #digits = [any(char.isdigit() for char in token) for token in tokentext] #any char contains digit\n",
    "\n",
    "            ## contains symbols (true), other characters\n",
    "            symbols = np.any([not token.isalnum() for token in tokentext])\n",
    "\n",
    "            ## contains time indicators ('yesterday','today')\n",
    "            time_indicator_list = ['yesterday','today','tomorrow']\n",
    "            \n",
    "            # note that I have already TIME as a NER tag below. However here I follow the paper.\n",
    "            times = np.any([True if token in time_indicator_list else False for token in tokentext])\n",
    "            \n",
    "            return [digits,symbols,times] #{'digits':digits,'symbols':symbols,'times':times}\n",
    "        \n",
    "        return np.array([extract_other_lexical_features_int(sentence) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NER_types = ['ORGANIZATION','PERSON','LOCATION','DATE','TIME','MONEY','PERCENT','FACILITY','FACILITY']\n",
    "NER_TYPES_spacy_all = ['PERSON','NORP','FAC','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW','LANGUAGE','DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']\n",
    "#NER_TYPES_spacy_subset = ['ORG','PERSON','LOC','GPE','DATE','TIME','MONEY','PERCENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    " # load spacy's English language models\n",
    "#en_nlp = spacy.load('en')\n",
    "en_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag map: https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tag_map.py with meaning: https://www.clips.uantwerpen.be/pages/mbsp-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_nlp.tokenizer.vocab.morphology.tag_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18 NER entities\n",
    "\n",
    "https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! cat ~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/spacy/lang/en/tag_map.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.tag_map import TAG_MAP\n",
    "#TAG_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract_syntactic_features(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    each sub-feature vector is of length all_pos_tags, fixed vector lengths!\n",
    "    '''\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self    \n",
    "\n",
    "    def transform(self, sentences):\n",
    "        '''\n",
    "        PoS Tagging and NER of sentence\n",
    "        '''\n",
    "        tags_docs = []\n",
    "        ner_docs = []\n",
    "        for doc in en_nlp.pipe(sentences): #, disable=[\"tagger\", \"parser\"]):\n",
    "            \n",
    "            # Do something with the doc here\n",
    "            #print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "            \n",
    "            tags = [token.pos_ for token in doc]\n",
    "            tags_docs.append(tags)\n",
    "            \n",
    "            ents = [ent.label_ for ent in doc.ents]\n",
    "            ner_docs.append(ents)\n",
    "\n",
    "        unique_tags = list(set(x for l in tags_docs for x in l))\n",
    "        unique_ner = list(set(x for l in ner_docs for x in l))\n",
    "        \n",
    "        print(\"unique tags: \",unique_tags)\n",
    "        print('uniquener: ', unique_ner)\n",
    "        \n",
    "        docs_features = []\n",
    "        for tags,ners in zip(tags_docs,ner_docs):\n",
    "            \n",
    "            tag_occurance = [apt in tags for apt in unique_tags]\n",
    "            \n",
    "            count_dict = Counter(tags)\n",
    "            \n",
    "            # number of occurances\n",
    "            tag_counts = [count_dict[apt] if apt in count_dict.keys() else 0 for apt in unique_tags]\n",
    "            \n",
    "            # occurance, 0, 1 or more\n",
    "            tag_three_classes = [2 if tc>1 else tc for tc in tag_counts]\n",
    "        \n",
    "            \n",
    "            # named entity recognition: person, organization, location, product, event,\n",
    "                    \n",
    "            ners_feature = [1 if ner in ners else 0 for ner in unique_ner]\n",
    "            \n",
    "            sent_features = tag_occurance+tag_three_classes+tag_three_classes #{'tag_occurance':tag_occurance,'tag_three_classes':tag_three_classes,'ners':ners}\n",
    "            docs_features.append(sent_features)\n",
    "            \n",
    "        return np.array(docs_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lemmatize(sentence):\n",
    "    \n",
    "    #tokentext = nltk.word_tokenize(sentence)\n",
    "    return [token.lemma_ for token in en_nlp(sentence)]\n",
    "\n",
    "def tokenize_lemma_pos(sentence):\n",
    "    '''\n",
    "    Combine token name and pos label\n",
    "    '''\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for token in en_nlp(sentence):\n",
    "            tokens.append(token.lemma_+'_'+token.pos_)\n",
    "\n",
    "    #tokentext = nltk.word_tokenize(sentence)\n",
    "    return tokens #[en_nlp(token[0])[0].lemma_+token[1] for token in nltk.pos_tag(tokentext)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.shape = X.shape\n",
    "        # what other output you want\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sentence = 'The New York Times posted about people running marathons'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    \n",
    "   # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            \n",
    "            # Pipeline for getting syntactic features\n",
    "            ('syntactic_features', Pipeline([\n",
    "                ('extract_syntactic_features', extract_syntactic_features()),\n",
    "                (\"debug\", Debug()),\n",
    "\n",
    "                #('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),    \n",
    "    \n",
    "\n",
    "            # Pipeline for getting other lexical features\n",
    "            ('other_lexical_features', Pipeline([\n",
    "                ('extract_other_lexical_features', extract_other_lexical_features()),\n",
    "                #('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "                (\"debug\", Debug()),\n",
    "\n",
    "            ])),               \n",
    "    \n",
    "            # word token ngrams\n",
    "            ('word_ngrams', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,3),analyzer='word')),\n",
    "                (\"debug\", Debug())\n",
    "            ])),\n",
    "    \n",
    "    \n",
    "            # character token ngrams\n",
    "            ('char_ngrams', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(3,4),analyzer='char')),\n",
    "                (\"debug\", Debug())\n",
    "            ])),     \n",
    "    \n",
    "            \n",
    "    \n",
    "            ## lemma n-gram features, MODIFIED tokenizer=tokenize_lemmatize,\n",
    "            ('lemma_ngrams', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,3),tokenizer=tokenize_lemmatize,analyzer='word')),\n",
    "                (\"debug\", Debug())\n",
    "            ])),              \n",
    "    \n",
    "            \n",
    "            \n",
    "            ## Get lemma + POS tags, MODIFIED, tokenizer=tokenize_lemma_pos,\n",
    "            ('lemma_pos', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(analyzer='word',tokenizer=tokenize_lemma_pos)),\n",
    "                (\"debug\", Debug()),\n",
    "            ]))     \n",
    "            \n",
    "\n",
    "        ]\n",
    "        \n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "        #    'subject': 1.0,\n",
    "        #    'body_bow': 1.0,\n",
    "        #    'body_stats': 1.0,\n",
    "        #},\n",
    "    )),\n",
    "    # Use a SVC classifier on the combined features\n",
    "    #('svc', SVC(kernel='linear')),\n",
    "    \n",
    "    (\"debug_final\", Debug()),\n",
    "\n",
    "    ('svc',OneVsRestClassifier(SVC(kernel='linear'))),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_corpus = './Data/jacobs_corpus.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(path_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>datatype</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>file_id</th>\n",
       "      <th>-1</th>\n",
       "      <th>Profit</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>MergerAcquisition</th>\n",
       "      <th>SalesVolume</th>\n",
       "      <th>BuyRating</th>\n",
       "      <th>QuarterlyResults</th>\n",
       "      <th>TargetPrice</th>\n",
       "      <th>ShareRepurchase</th>\n",
       "      <th>Turnover</th>\n",
       "      <th>Debt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It will not say what it has spent on the proje...</td>\n",
       "      <td>-1</td>\n",
       "      <td>holdin</td>\n",
       "      <td>tesco</td>\n",
       "      <td>25-09-2013</td>\n",
       "      <td>833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sir John Bond , chairman , told the bank 's an...</td>\n",
       "      <td>-1</td>\n",
       "      <td>holdin</td>\n",
       "      <td>FT other HSBC</td>\n",
       "      <td>28-05-2005</td>\n",
       "      <td>393</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence label datatype  \\\n",
       "0  It will not say what it has spent on the proje...    -1   holdin   \n",
       "1  Sir John Bond , chairman , told the bank 's an...    -1   holdin   \n",
       "\n",
       "           title publication_date  file_id  -1  Profit  Dividend  \\\n",
       "0          tesco       25-09-2013      833   1       0         0   \n",
       "1  FT other HSBC       28-05-2005      393   1       0         0   \n",
       "\n",
       "   MergerAcquisition  SalesVolume  BuyRating  QuarterlyResults  TargetPrice  \\\n",
       "0                  0            0          0                 0            0   \n",
       "1                  0            0          0                 0            0   \n",
       "\n",
       "   ShareRepurchase  Turnover  Debt  \n",
       "0                0         0     0  \n",
       "1                0         0     0  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_labels = ['-1',\n",
    "       'Profit', 'Dividend', 'MergerAcquisition', 'SalesVolume', 'BuyRating',\n",
    "       'QuarterlyResults', 'TargetPrice', 'ShareRepurchase', 'Turnover',\n",
    "       'Debt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9937,), (9937, 11))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['sentence']\n",
    "y = df[multi_labels]\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[231,233],[123],[2,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 231, 233, 123]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(x for l in a for x in l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tags:  ['DET', 'INTJ', 'NOUN', 'AUX', 'VERB', 'PROPN', 'ADP', 'NUM', 'SYM', 'PART', 'PUNCT', 'PRON', 'ADV', 'SCONJ', 'ADJ', 'X', 'CCONJ', 'SPACE']\n",
      "uniquener:  ['PERSON', 'ORG', 'MONEY', 'FAC', 'NORP', 'LANGUAGE', 'WORK_OF_ART', 'CARDINAL', 'GPE', 'TIME', 'LOC', 'PRODUCT', 'QUANTITY', 'DATE', 'LAW', 'PERCENT', 'ORDINAL', 'EVENT']\n",
      "CPU times: user 7min 51s, sys: 9.87 s, total: 8min 1s\n",
      "Wall time: 8min 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('union',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('syntactic_features',\n",
       "                                                 Pipeline(memory=None,\n",
       "                                                          steps=[('extract_syntactic_features',\n",
       "                                                                  extract_syntactic_features()),\n",
       "                                                                 ('debug',\n",
       "                                                                  Debug())],\n",
       "                                                          verbose=False)),\n",
       "                                                ('other_lexical_features',\n",
       "                                                 Pipeline(memory=None,\n",
       "                                                          steps=[('extract_other_lexical_features',\n",
       "                                                                  extract_other_lexical_features()),\n",
       "                                                                 ('d...\n",
       "                              transformer_weights=None, verbose=False)),\n",
       "                ('debug_final', Debug()),\n",
       "                ('svc',\n",
       "                 OneVsRestClassifier(estimator=SVC(C=1.0, break_ties=False,\n",
       "                                                   cache_size=200,\n",
       "                                                   class_weight=None, coef0=0.0,\n",
       "                                                   decision_function_shape='ovr',\n",
       "                                                   degree=3, gamma='scale',\n",
       "                                                   kernel='linear', max_iter=-1,\n",
       "                                                   probability=False,\n",
       "                                                   random_state=None,\n",
       "                                                   shrinking=True, tol=0.001,\n",
       "                                                   verbose=False),\n",
       "                                     n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.steps\n",
    "#pipeline.get_params()['svc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('syntactic_features: ',\n",
       " (6657, 54),\n",
       " 'other_lexical_features: ',\n",
       " (6657, 3),\n",
       " 'word_ngrams: ',\n",
       " (6657, 176571),\n",
       " 'char_ngrams: ',\n",
       " (6657, 43589),\n",
       " 'lemma_ngrams: ',\n",
       " (6657, 183671),\n",
       " 'lemma_pos: ',\n",
       " (6657, 11202),\n",
       " 'total dim: ',\n",
       " (6657, 415090))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'syntactic_features: ',pipeline.get_params()['union'].get_params()['syntactic_features'].get_params()['debug'].shape, \\\n",
    "'other_lexical_features: ',pipeline.get_params()['union'].get_params()['other_lexical_features'].get_params()['debug'].shape, \\\n",
    "'word_ngrams: ',pipeline.get_params()['union'].get_params()['word_ngrams'].get_params()['debug'].shape,\\\n",
    "'char_ngrams: ',pipeline.get_params()['union'].get_params()['char_ngrams'].get_params()['debug'].shape,\\\n",
    "'lemma_ngrams: ',pipeline.get_params()['union'].get_params()['lemma_ngrams'].get_params()['debug'].shape, \\\n",
    "'lemma_pos: ',pipeline.get_params()['union'].get_params()['lemma_pos'].get_params()['debug'].shape, \\\n",
    "'total dim: ',pipeline.get_params()['debug_final'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6657, 54),\n",
       " (6657, 3),\n",
       " (6657, 10264),\n",
       " (6657, 176571),\n",
       " (6657, 43589),\n",
       " (6657, 176571))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'syntactic_features: ',pipeline.get_params()['union'].get_params()['syntactic_features'].get_params()['debug'].shape, \\\n",
    "'other_lexical_features: ',pipeline.get_params()['union'].get_params()['other_lexical_features'].get_params()['debug'].shape, \\\n",
    "'word_ngrams: ',pipeline.get_params()['union'].get_params()['word_ngrams'].get_params()['debug'].shape,\\\n",
    "'char_ngrams: ',pipeline.get_params()['union'].get_params()['char_ngrams'].get_params()['debug'].shape,\\\n",
    "'lemma_ngrams: ',pipeline.get_params()['union'].get_params()['lemma_ngrams'].get_params()['debug'].shape\n",
    "'lemma_pos: ',pipeline.get_params()['union'].get_params()['lemma_pos'].get_params()['debug'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 400k features per data point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tags:  ['DET', 'INTJ', 'NOUN', 'AUX', 'VERB', 'PROPN', 'ADP', 'NUM', 'SYM', 'PART', 'PUNCT', 'PRON', 'SCONJ', 'ADV', 'ADJ', 'X', 'CCONJ', 'SPACE']\n",
      "uniquener:  ['PERSON', 'ORG', 'MONEY', 'NORP', 'FAC', 'WORK_OF_ART', 'CARDINAL', 'GPE', 'LOC', 'TIME', 'PRODUCT', 'QUANTITY', 'DATE', 'LAW', 'PERCENT', 'ORDINAL', 'EVENT']\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "               -1       0.93      0.95      0.94      2593\n",
      "           Profit       0.82      0.77      0.80       218\n",
      "         Dividend       0.74      0.70      0.72        50\n",
      "MergerAcquisition       0.73      0.23      0.36        81\n",
      "      SalesVolume       0.81      0.69      0.75       143\n",
      "        BuyRating       0.93      0.76      0.84        72\n",
      " QuarterlyResults       0.64      0.69      0.67        88\n",
      "      TargetPrice       0.87      0.93      0.90        28\n",
      "  ShareRepurchase       0.80      0.31      0.44        26\n",
      "         Turnover       0.90      0.68      0.77        77\n",
      "             Debt       0.67      0.40      0.50        20\n",
      "\n",
      "        micro avg       0.90      0.88      0.89      3396\n",
      "        macro avg       0.80      0.65      0.70      3396\n",
      "     weighted avg       0.90      0.88      0.88      3396\n",
      "      samples avg       0.88      0.89      0.88      3396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christian/opt/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred,target_names=multi_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.externals import joblib\n",
    "#joblib.dump(pipeline,'../Models/TrainingJacobs/model.joblib',compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('py36': conda)",
   "language": "python",
   "name": "python36764bitpy36conda5bb0a2a1b1794bc58c6e05827e429966"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
