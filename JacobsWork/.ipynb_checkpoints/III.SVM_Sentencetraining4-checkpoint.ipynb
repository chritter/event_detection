{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. SVM classification 4\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "* AS  III. SVM_Sentencetraining3 but focus on M&A only\n",
    "* Adapted to work with latest version of spacy (03/20)\n",
    "\n",
    "### Comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical features\n",
    "\n",
    "* token n-gram features: unigrams,bigrams, trigrams\n",
    "* character n-gram fatures: trigrams,fourgrams\n",
    "* lemma n-gram features: uni,bi,trigrams\n",
    "* disambiguated lemmas: Lemma + POS tag\n",
    "* numerals: yes,no\n",
    "* symbols: yes,no\n",
    "* time indicators: yes, not\n",
    "* future: add semantic knowledge from structured resources:\n",
    "    * takeover=acquire, acquisition,\n",
    "    * are word embedding sufficient to capture semantic knowledge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic features\n",
    "* PoS categories: \n",
    "    * for each binary (yes,no)\n",
    "    * 0,1,more; \n",
    "    * total number of occurances\n",
    "* named entity types: person, organization, location, product, event, \n",
    "\n",
    "\n",
    "    NE Type \tExamples\n",
    "    ORGANIZATION \tGeorgia-Pacific Corp., WHO\n",
    "    PERSON \tEddy Bonte, President Obama\n",
    "    LOCATION \tMurray River, Mount Everest\n",
    "    DATE \tJune, 2008-06-29\n",
    "    TIME \ttwo fifty a m, 1:30 p.m.\n",
    "    MONEY \t175 million Canadian Dollars, GBP 10.40\n",
    "    PERCENT \ttwenty pct, 18.75 %\n",
    "    FACILITY \tWashington Monument, Stonehenge\n",
    "    GPE \tSouth East Asia, Midlothian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction on sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk,gensim, spacy\n",
    "\n",
    "#nltk.data.path=[]\n",
    "#nltk.data.path.append(\"C:\\\\Users\\\\rittchr\\\\nltk_data\")\n",
    "#nltk.data.path.append(\"\\\\esdfiles\\INTERNAL\\SpecialProjects\\EconomicEventDetection\\Analytics\\nltk_data\")\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('tagsets')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.data import load\n",
    "#all_pos_tags = list(load('help/tagsets/upenn_tagset.pickle').keys())\n",
    "#all_pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract_other_lexical_features(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    other lexical features such as time, special chars\n",
    "    '''\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self    \n",
    "\n",
    "    def transform(self, sentences):\n",
    "    \n",
    "        def extract_other_lexical_features_int(sentence):\n",
    "            '''\n",
    "            Simple indicator variables if digits, symobls or times are mentioned in the sentence\n",
    "            '''\n",
    "        \n",
    "            tokentext = nltk.word_tokenize(sentence)\n",
    "\n",
    "            ## Check if it is digit, could also use POS tag 'NUM'\n",
    "            digits = np.any([token.isdigit() for token in tokentext])\n",
    "            #digits = [any(char.isdigit() for char in token) for token in tokentext] #any char contains digit\n",
    "\n",
    "            ## contains symbols (true), other characters\n",
    "            symbols = np.any([not token.isalnum() for token in tokentext])\n",
    "\n",
    "            ## contains time indicators ('yesterday','today')\n",
    "            time_indicator_list = ['yesterday','today','tomorrow']\n",
    "            \n",
    "            # note that I have already TIME as a NER tag below. However here I follow the paper.\n",
    "            times = np.any([True if token in time_indicator_list else False for token in tokentext])\n",
    "            \n",
    "            return [digits,symbols,times] #{'digits':digits,'symbols':symbols,'times':times}\n",
    "        \n",
    "        return np.array([extract_other_lexical_features_int(sentence) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NER_types = ['ORGANIZATION','PERSON','LOCATION','DATE','TIME','MONEY','PERCENT','FACILITY','FACILITY']\n",
    "NER_TYPES_spacy_all = ['PERSON','NORP','FAC','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW','LANGUAGE','DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']\n",
    "#NER_TYPES_spacy_subset = ['ORG','PERSON','LOC','GPE','DATE','TIME','MONEY','PERCENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " # load spacy's English language models\n",
    "#en_nlp = spacy.load('en')\n",
    "en_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag map: https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tag_map.py with meaning: https://www.clips.uantwerpen.be/pages/mbsp-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_nlp.tokenizer.vocab.morphology.tag_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18 NER entities\n",
    "\n",
    "https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! cat ~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/spacy/lang/en/tag_map.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.tag_map import TAG_MAP\n",
    "#TAG_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract_syntactic_features(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    each sub-feature vector is of length all_pos_tags, fixed vector lengths!\n",
    "    '''\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self    \n",
    "\n",
    "    def transform(self, sentences):\n",
    "        '''\n",
    "        PoS Tagging and NER of sentence\n",
    "        '''\n",
    "        tags_docs = []\n",
    "        ner_docs = []\n",
    "        for doc in en_nlp.pipe(sentences): #, disable=[\"tagger\", \"parser\"]):\n",
    "            \n",
    "            # Do something with the doc here\n",
    "            #print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "            \n",
    "            tags = [token.pos_ for token in doc]\n",
    "            tags_docs.append(tags)\n",
    "            \n",
    "            ents = [ent.label_ for ent in doc.ents]\n",
    "            ner_docs.append(ents)\n",
    "\n",
    "        unique_tags = list(set(x for l in tags_docs for x in l))\n",
    "        unique_ner = list(set(x for l in ner_docs for x in l))\n",
    "        \n",
    "        print(\"unique tags: \",unique_tags)\n",
    "        print('uniquener: ', unique_ner)\n",
    "        \n",
    "        docs_features = []\n",
    "        for tags,ners in zip(tags_docs,ner_docs):\n",
    "            \n",
    "            tag_occurance = [apt in tags for apt in unique_tags]\n",
    "            \n",
    "            count_dict = Counter(tags)\n",
    "            \n",
    "            # number of occurances\n",
    "            tag_counts = [count_dict[apt] if apt in count_dict.keys() else 0 for apt in unique_tags]\n",
    "            \n",
    "            # occurance, 0, 1 or more\n",
    "            tag_three_classes = [2 if tc>1 else tc for tc in tag_counts]\n",
    "        \n",
    "            \n",
    "            # named entity recognition: person, organization, location, product, event,\n",
    "                    \n",
    "            ners_feature = [1 if ner in ners else 0 for ner in unique_ner]\n",
    "            \n",
    "            sent_features = tag_occurance+tag_three_classes+tag_three_classes #{'tag_occurance':tag_occurance,'tag_three_classes':tag_three_classes,'ners':ners}\n",
    "            docs_features.append(sent_features)\n",
    "            \n",
    "        return np.array(docs_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lemmatize(sentence):\n",
    "    \n",
    "    #tokentext = nltk.word_tokenize(sentence)\n",
    "    return [token.lemma_ for token in en_nlp(sentence)]\n",
    "\n",
    "def tokenize_lemma_pos(sentence):\n",
    "    '''\n",
    "    Combine token name and pos label\n",
    "    '''\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for token in en_nlp(sentence):\n",
    "            tokens.append(token.lemma_+'_'+token.pos_)\n",
    "\n",
    "    #tokentext = nltk.word_tokenize(sentence)\n",
    "    return tokens #[en_nlp(token[0])[0].lemma_+token[1] for token in nltk.pos_tag(tokentext)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.shape = X.shape\n",
    "        # what other output you want\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sentence = 'The New York Times posted about people running marathons'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    \n",
    "   # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            \n",
    "            # Pipeline for getting syntactic features\n",
    "            ('syntactic_features', Pipeline([\n",
    "                ('extract_syntactic_features', extract_syntactic_features()),\n",
    "                (\"debug\", Debug()),\n",
    "\n",
    "                #('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),    \n",
    "    \n",
    "\n",
    "            # Pipeline for getting other lexical features\n",
    "            ('other_lexical_features', Pipeline([\n",
    "                ('extract_other_lexical_features', extract_other_lexical_features()),\n",
    "                #('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "                (\"debug\", Debug()),\n",
    "\n",
    "            ])),               \n",
    "    \n",
    "            # word token ngrams\n",
    "            ('word_ngrams', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,3),analyzer='word')),\n",
    "                (\"debug\", Debug())\n",
    "            ])),\n",
    "    \n",
    "    \n",
    "            # character token ngrams\n",
    "            ('char_ngrams', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(3,4),analyzer='char')),\n",
    "                (\"debug\", Debug())\n",
    "            ])),     \n",
    "    \n",
    "            \n",
    "    \n",
    "            ## lemma n-gram features, MODIFIED tokenizer=tokenize_lemmatize,\n",
    "            ('lemma_ngrams', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,3),tokenizer=tokenize_lemmatize,analyzer='word')),\n",
    "                (\"debug\", Debug())\n",
    "            ])),              \n",
    "    \n",
    "            \n",
    "            \n",
    "            ## Get lemma + POS tags, MODIFIED, tokenizer=tokenize_lemma_pos,\n",
    "            ('lemma_pos', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(analyzer='word',tokenizer=tokenize_lemma_pos)),\n",
    "                (\"debug\", Debug()),\n",
    "            ]))     \n",
    "            \n",
    "\n",
    "        ]\n",
    "        \n",
    "        \n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        #transformer_weights={\n",
    "        #    'subject': 1.0,\n",
    "        #    'body_bow': 1.0,\n",
    "        #    'body_stats': 1.0,\n",
    "        #},\n",
    "    )),\n",
    "    \n",
    "    (\"debug_final\", Debug()),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    ('svc', SVC(kernel='linear')),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_corpus = './Data/jacobs_corpus.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(path_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>datatype</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>file_id</th>\n",
       "      <th>-1</th>\n",
       "      <th>Profit</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>MergerAcquisition</th>\n",
       "      <th>SalesVolume</th>\n",
       "      <th>BuyRating</th>\n",
       "      <th>QuarterlyResults</th>\n",
       "      <th>TargetPrice</th>\n",
       "      <th>ShareRepurchase</th>\n",
       "      <th>Turnover</th>\n",
       "      <th>Debt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It will not say what it has spent on the proje...</td>\n",
       "      <td>-1</td>\n",
       "      <td>holdin</td>\n",
       "      <td>tesco</td>\n",
       "      <td>25-09-2013</td>\n",
       "      <td>833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sir John Bond , chairman , told the bank 's an...</td>\n",
       "      <td>-1</td>\n",
       "      <td>holdin</td>\n",
       "      <td>FT other HSBC</td>\n",
       "      <td>28-05-2005</td>\n",
       "      <td>393</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence label datatype  \\\n",
       "0  It will not say what it has spent on the proje...    -1   holdin   \n",
       "1  Sir John Bond , chairman , told the bank 's an...    -1   holdin   \n",
       "\n",
       "           title publication_date  file_id  -1  Profit  Dividend  \\\n",
       "0          tesco       25-09-2013      833   1       0         0   \n",
       "1  FT other HSBC       28-05-2005      393   1       0         0   \n",
       "\n",
       "   MergerAcquisition  SalesVolume  BuyRating  QuarterlyResults  TargetPrice  \\\n",
       "0                  0            0          0                 0            0   \n",
       "1                  0            0          0                 0            0   \n",
       "\n",
       "   ShareRepurchase  Turnover  Debt  \n",
       "0                0         0     0  \n",
       "1                0         0     0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_labels = ['-1',\n",
    "       'Profit', 'Dividend', 'MergerAcquisition', 'SalesVolume', 'BuyRating',\n",
    "       'QuarterlyResults', 'TargetPrice', 'ShareRepurchase', 'Turnover',\n",
    "       'Debt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_label = 'MergerAcquisition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9937,), (9937,))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['sentence']\n",
    "y = df[one_label]\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,file_id_train,file_id_test = train_test_split(X, y, df['file_id'],test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[231,233],[123],[2,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 231, 233, 123]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(x for l in a for x in l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.steps\n",
    "#pipeline.get_params()['svc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'syntactic_features: ',pipeline.get_params()['union'].get_params()['syntactic_features'].get_params()['debug'].shape, \\\n",
    "'other_lexical_features: ',pipeline.get_params()['union'].get_params()['other_lexical_features'].get_params()['debug'].shape, \\\n",
    "'word_ngrams: ',pipeline.get_params()['union'].get_params()['word_ngrams'].get_params()['debug'].shape,\\\n",
    "'char_ngrams: ',pipeline.get_params()['union'].get_params()['char_ngrams'].get_params()['debug'].shape,\\\n",
    "'lemma_ngrams: ',pipeline.get_params()['union'].get_params()['lemma_ngrams'].get_params()['debug'].shape, \\\n",
    "'lemma_pos: ',pipeline.get_params()['union'].get_params()['lemma_pos'].get_params()['debug'].shape, \\\n",
    "'total dim: ',pipeline.get_params()['debug_final'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 400k features per data point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred,target_names=multi_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.externals import joblib\n",
    "#joblib.dump(pipeline,'../Models/TrainingJacobs/model.joblib',compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('py36': conda)",
   "language": "python",
   "name": "python36764bitpy36conda5bb0a2a1b1794bc58c6e05827e429966"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
